#version 460

#extension GL_GOOGLE_include_directive: require
#extension GL_EXT_mesh_shader: require
#extension GL_EXT_debug_printf : require
#extension GL_EXT_control_flow_attributes : require
#extension GL_KHR_shader_subgroup_shuffle : require

#include "payload.h"

const uint WORK_GROUP_SIZE = 8;

layout (local_size_x = WORK_GROUP_SIZE, local_size_y = WORK_GROUP_SIZE) in;

// Each mesh shader does at least a quadrant of a patch
layout (triangles, max_vertices = 64, max_primitives = 98) out;

// Inputs
taskPayloadSharedEXT Payload payload;

layout (push_constant) uniform NGFPushConstants {
	mat4 model;
	mat4 view;
	mat4 proj;

	vec2 extent;
	float time;
};

layout (binding = 0) readonly buffer Points
{
	vec3 data[];
} points;

layout (binding = 1) readonly buffer Features
{
	float data[];
} features;

layout (binding = 2) readonly buffer Patches
{
	ivec4 data[];
} patches;

// Local execution data
const uint ENCODING_LEVELS = 8;
const uint FFIN            = FEATURE_SIZE + 3 * 2 * ENCODING_LEVELS;

// Neural network weights
layout (binding = 4) uniform sampler1D biases;
layout (binding = 5) uniform sampler2D w0;
layout (binding = 6) uniform sampler2D w1;
layout (binding = 7) uniform sampler2D w2;
layout (binding = 8) uniform sampler2D w3;

// Outputs
layout (location = 0) out vec3 position[];
layout (location = 2) out flat uint pindex[];

vec4 project(vec3 p)
{
	vec4 pp = proj * view * model * vec4(p, 1.0f);
	pp.y = -pp.y;
	pp.z = (pp.z + pp.w) / 2.0;
	return pp;
}

#define leaky_relu(x) max(x, 0.01 * x)

vec3 eval(ivec4 complex, float u, float v)
{
	const uint MSIZE = max(FFIN, 64);

	float A[MSIZE];
	float B[64];

	vec3 v0 = points.data[complex.x];
	vec3 v1 = points.data[complex.y];
	vec3 v2 = points.data[complex.z];
	vec3 v3 = points.data[complex.w];

	vec3 vertex = mix(mix(v0, v1, v), mix(v3, v2, v), u);

	// TODO: align as a vec4
	for (uint i = 0; i < FEATURE_SIZE; i++) {
		float f0 = features.data[complex.x * FEATURE_SIZE + i];
		float f1 = features.data[complex.y * FEATURE_SIZE + i];
		float f2 = features.data[complex.z * FEATURE_SIZE + i];
		float f3 = features.data[complex.w * FEATURE_SIZE + i];
		A[i] = mix(mix(f0, f1, v), mix(f3, f2, v), u);
	}

	// Positional encoding
	uint k = FEATURE_SIZE;
	for (uint i = 0; i < ENCODING_LEVELS; i++) {
		float p = pow(2, i);
		vec3 sin_v = sin(p * vertex);
		vec3 cos_v = cos(p * vertex);

		A[k++] = sin_v.x;
		A[k++] = sin_v.y;
		A[k++] = sin_v.z;

		A[k++] = cos_v.x;
		A[k++] = cos_v.y;
		A[k++] = cos_v.z;
	}

	// Network evaluation

	// Layer 0
	for (uint i = 0; i < 16; i++) {
		uint k = 4 * i;

		vec4 v = texelFetch(biases, int(i), 0);
		for (uint j = 0; j < FFIN; j++)
			v += A[j] * texelFetch(w0, ivec2(i, j), 0);

		vec4 lv = leaky_relu(v);
		B[k + 0] = lv.x;
		B[k + 1] = lv.y;
		B[k + 2] = lv.z;
		B[k + 3] = lv.w;
	}

	// Layer 1
	for (uint i = 0; i < 16; i++) {
		uint k = 4 * i;

		vec4 v = texelFetch(biases, int(i) + 16, 0);
		for (uint j = 0; j < 64; j++)
			v += B[j] * texelFetch(w1, ivec2(i, j), 0);

		vec4 lv = leaky_relu(v);
		A[k + 0] = lv.x;
		A[k + 1] = lv.y;
		A[k + 2] = lv.z;
		A[k + 3] = lv.w;
	}

	// Layer 2
	for (uint i = 0; i < 16; i++) {
		uint k = 4 * i;

		vec4 v = texelFetch(biases, int(i) + 2 * 16, 0);
		for (uint j = 0; j < 64; j++)
			v += A[j] * texelFetch(w2, ivec2(i, j), 0);

		vec4 lv = leaky_relu(v);
		B[k + 0] = lv.x;
		B[k + 1] = lv.y;
		B[k + 2] = lv.z;
		B[k + 3] = lv.w;

		// TODO: fuse with the last layer?
	}

	// Layer 3
	[[unroll]]
	for (uint i = 0; i < 3; i++) {
		float sum = 0.0f;
		for (uint j = 0; j < 16; j++) {
			uint k = 4 * j;
			vec4 w = texelFetch(w3, ivec2(j, i), 0);
			vec4 v = vec4(B[k], B[k + 1], B[k + 2], B[k + 3]);
			sum += dot(v, w);
		}
		vertex[i] += sum;
	}

	return vertex + texelFetch(biases, 3 * 64, 0).xyz;
}

void main()
{
	const uint MAX_QSIZE = WORK_GROUP_SIZE - 1;

	// TODO: need dyanmic QSIZE/stride depending on resolution
	uvec2 offset = gl_LocalInvocationID.xy + (MAX_QSIZE * gl_WorkGroupID.xy);
	if (offset.x > payload.resolution || offset.y > payload.resolution)
		return;

	uvec2 offset_triangles = MAX_QSIZE * gl_WorkGroupID.xy;

	uint total_qsize = payload.resolution - 1;
	uint qwidth = min(MAX_QSIZE, total_qsize - offset_triangles.x);
	uint qheight = min(MAX_QSIZE, total_qsize - offset_triangles.y);

	uint vwidth = qwidth + 1;
	uint vheight = qheight + 1;
	SetMeshOutputsEXT(vwidth * vheight, 2 * qwidth * qheight);

	vec2 uv = offset/float(payload.resolution - 1);
	vec3 v = eval(patches.data[payload.pindex], uv.x, uv.y);

	gl_MeshVerticesEXT[gl_LocalInvocationIndex].gl_Position = project(v);

	// Send position to fragment shader for normal vector calculations
	position[gl_LocalInvocationIndex] = vec3(model * vec4(v, 1.0f));
	pindex[gl_LocalInvocationIndex] = payload.pindex;

	uint gli = gl_LocalInvocationIndex;
	if (gl_LocalInvocationID.x < qwidth && gl_LocalInvocationID.y < qheight) {
		uint prim = 2 * (gl_LocalInvocationID.x + qwidth * gl_LocalInvocationID.y);

		// Assumes that the subgroup size is 32
		uint gsi = gl_SubgroupInvocationID;

		float sign = 1;
		uint side = gsi + 1;
		if (side >= 32 || gl_LocalInvocationID.x >= qwidth) {
			side = gsi - 1;
			sign *= -1;
		}

		uint vert = gsi + WORK_GROUP_SIZE;
		if (vert >= 32) {
			vert = gsi - WORK_GROUP_SIZE;
			sign *= -1;
		}

		uint sidevert = gsi + WORK_GROUP_SIZE + 1;
		if (sidevert >= 32)
			sidevert = gsi - WORK_GROUP_SIZE + 1;
		if (sidevert >= 32)
			sidevert = gsi - WORK_GROUP_SIZE - 1;

		vec3 sv = subgroupShuffle(v, side);
		vec3 vv = subgroupShuffle(v, vert);
		vec3 svv = subgroupShuffle(v, sidevert);

		float d0 = length(v - svv);
		float d1 = length(sv - vv);

		if (d0 > d1) {
			gl_PrimitiveTriangleIndicesEXT[prim] = uvec3(gli, gli + 1, gli + WORK_GROUP_SIZE);
			gl_PrimitiveTriangleIndicesEXT[prim + 1] = uvec3(gli + 1, gli + WORK_GROUP_SIZE + 1, gli + WORK_GROUP_SIZE);
		} else {
			gl_PrimitiveTriangleIndicesEXT[prim] = uvec3(gli, gli + 1, gli + WORK_GROUP_SIZE + 1);
			gl_PrimitiveTriangleIndicesEXT[prim + 1] = uvec3(gli, gli + WORK_GROUP_SIZE + 1, gli + WORK_GROUP_SIZE);

		}
	}
}
